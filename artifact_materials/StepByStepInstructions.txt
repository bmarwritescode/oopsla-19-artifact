Step By Step Instructions: Program Synthesis with Algebraic Library Specifications
-----------------------------------------------------------------------------------
-----------------------------------------------------------------------------------

Benchmarks
-----------

- You can find the benchmarks (described in Table 1) in	`test/axioms/benchmarks`.
- In this directory, each folder contains a different benchmark. For each benchmark, the synthesis problem is contained in the file denoted "NAME_syn.java" while the solution is simply "NAME.java". All shared libraries (that is libraries used with both specs and mocks) are contained in the `shared/` folder. All specs are contained in the `rewrite/` folder, and all mocks are contained in the `mocks/` folder.
- For the cryptography examples, please note the "_mock" and "_rewrite" naming scheme which differentiates the versions used for mocks and rewrites. This was necessary due to annotations, as discussed in Section 4 of the paper.

LOC Comparison (Section 5.1)
-----------------------------

- One can manually compare the algebraic specifications versus the mocks for various libraries by looking in the `rewrite/` and `mock/` folders for the benchmarks. One should see, as described in section 5.1, that both implement the same API.
- To compare the lines of code used for each approach, we have preinstalled the SLOCCount tool. To run the comparison, simply run the `sloccounter.sh` script by running `./sloccounter.sh`. If for some reason it doesn't have execute permissions, give it them by running first `chmod +x sloccounter.sh`.
- The output of this will be stuck in the `sloccounts/` folder. Each benchmark will have its own file. We will pay attemtion to the output in the SLOC-by-Language column about halfway through the output. Here, copare the lines where the directory (second column) equals `mock` and `rewrite`. The number of lines of code for each of these are given in the third column by `java=XXX` where XXX is the number of reported LOC in the paper.

Synthesis Problems (Section 5.2)
---------------------------------

- In Table 2, we see a brief description of the synthesis problems, including the number of calls to the `stmts` and `guards` generators. This information can be verified by visiting the various `*_syn.java` files for each benchmark.

Performance Comparison (Section 5.2)
-------------------------------------

- In Table 2, we also give a performance comparison for mocks versus algebraic specifications.
- The script that runs this performance comparison is given in the file `test/run_jlibsketch_benchmarks.py`. Note that there are two methods per benchmark, one for the mocks and one for the specifications (denoted `_mock` and `_rewrite` respectively). The Sketch arguments for each are passed as arguments to the `run` method, which runs JLibSketch on the specified problem.
- While one could just run this script to verify the results, I would not suggest it. Note that 31 trials of each experiment was run, meaning a total of 551 experiments were run. Given that multiple tests take over an hour (and a few even timeout), this would take a very long time. One could reduce the number of trials, but even with just 1 trial, it would likely take many hours to run the experiments. Also note that inter-quartile-range (IQR) for many of the experiments is quite large, meaning performance for an individual test might vary significantly.
- Note also that we ran the experiments on a fairly powerful machine: 10 cores and 128 GB of RAM, as described in Section 5.2. As a result, while we would not expect the relative performance of mocks versus specifications to change, a significant (though likely proportional) slowing in both approaches may be observed on other machines.
- To run the benchmarks, one can navigate to `java-sketch/` and run the following command: `python -m test.jlibsketch_benchmarks`
- Similar to the examples from the Getting Started Guide, this script will output all of the intermediate sketch files in `/result/sk_BENCHMARK` where `BENCHMARK` is the name of the benchmark being run.
- Additionally, like the Getting Started Guide, the final output from Sketch with the completed synthesis solution can be found in `/result/output/BENCHMARK.txt`.
- In addition to these files, one can also observe the timing information summarized in two files: `rewrite_results.csv` and `mock_results.csv` which give the runtimes for experiments using rewrites as well as mocks. Runtime for an individual run can also be found at the bottom of `result/output/BENCHMARK.txt`.
- As running all of the benchmarks is not feasible, we have set up a similar running script, `test/short_jlibsketch_benchmarks.py`. This runs a few of the shorter benchmarks and finished in under 30 minutes on our machine. Run this script with: `python -m test.short_jlibsketch_benchmarks` from `java-sketch/`. Outputs of this result should be accessed in the same way as outputs from the full benchmark testing script.
